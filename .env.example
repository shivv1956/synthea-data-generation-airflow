# Apache Airflow Environment Configuration
# This file contains all environment variables for the Airflow Docker Compose setup
# Copy this file to .env and replace placeholder values with actual credentials

# Airflow Core Settings
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False

# Airflow Scheduler Settings (optimized for fast DAG detection)
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=10

# Airflow Webserver Settings
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True

# Airflow Logging
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# Airflow User ID (must match host user for volume permissions)
AIRFLOW_UID=50000

# PostgreSQL Database Configuration
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_PORT=5432

# Airflow Web UI Admin User (created on first run)
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin

# Airflow Database Migration (run on initialization)
_AIRFLOW_DB_UPGRADE=True

# AWS S3 Configuration for Patient Data Upload
# Change these values to match your AWS setup
AWS_S3_BUCKET=your-s3-bucket-name
AWS_S3_PREFIX=raw/fhir
AWS_CONN_ID=aws_default

# AWS Credentials (REQUIRED - Replace with your actual credentials)
# These are used by both Airflow S3 operators and Snowflake S3 stage
AWS_ACCESS_KEY_ID=YOUR_AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=YOUR_AWS_SECRET_ACCESS_KEY
AWS_REGION=us-east-1

# Airflow AWS Connection String
# Format: aws://ACCESS_KEY_ID:SECRET_ACCESS_KEY@?region_name=REGION
# For IAM role: aws://@?region_name=us-east-1
# In CI/CD: Use GitHub Secrets to inject these values
AIRFLOW_CONN_AWS_DEFAULT=aws://YOUR_AWS_ACCESS_KEY_ID:YOUR_AWS_SECRET_ACCESS_KEY@?region_name=us-east-1

# =============================================================================
# SNOWFLAKE CONFIGURATION (REQUIRED - UPDATE WITH YOUR CREDENTIALS)
# =============================================================================
# Snowflake Account Configuration
# Format for SNOWFLAKE_ACCOUNT: account_identifier.region.cloud
# Example: xy12345.us-east-1.aws or xy12345.us-east-1 or orgname-accountname
SNOWFLAKE_ACCOUNT=your-account-identifier
SNOWFLAKE_USER=your_snowflake_username
SNOWFLAKE_PASSWORD=your_snowflake_password
SNOWFLAKE_DATABASE=SYNTHEA
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
SNOWFLAKE_SCHEMA=RAW
SNOWFLAKE_ROLE=TRANSFORMER

# Airflow Snowflake Connection String (REQUIRED)
# Format: snowflake://user:password@account/database/schema?warehouse=warehouse&role=role&account=account
# Uses ${SNOWFLAKE_DATABASE} and ${SNOWFLAKE_SCHEMA} variables from above
AIRFLOW_CONN_SNOWFLAKE_DEFAULT=snowflake://${SNOWFLAKE_USER}:${SNOWFLAKE_PASSWORD}@${SNOWFLAKE_ACCOUNT}/${SNOWFLAKE_DATABASE}/${SNOWFLAKE_SCHEMA}?warehouse=${SNOWFLAKE_WAREHOUSE}&role=${SNOWFLAKE_ROLE}&account=${SNOWFLAKE_ACCOUNT}

# =============================================================================
# OPTIONAL SETTINGS
# =============================================================================
# Data Transformation Settings
# Set to 'true' to enable data transformations before S3 upload
ENABLE_TRANSFORMATIONS=false
