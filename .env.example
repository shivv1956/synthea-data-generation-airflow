# Apache Airflow Environment Configuration
# This file contains all environment variables for the Airflow Docker Compose setup
# Copy this file to .env and replace placeholder values with actual credentials

# Airflow Core Settings
AIRFLOW__CORE__EXECUTOR=LocalExecutor
AIRFLOW__CORE__LOAD_EXAMPLES=False
AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False

# Airflow Scheduler Settings (optimized for fast DAG detection)
AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL=30
AIRFLOW__SCHEDULER__MIN_FILE_PROCESS_INTERVAL=10

# Airflow Webserver Settings
AIRFLOW__WEBSERVER__EXPOSE_CONFIG=True

# Airflow Logging
AIRFLOW__LOGGING__LOGGING_LEVEL=INFO

# Airflow User ID (must match host user for volume permissions)
AIRFLOW_UID=50000

# PostgreSQL Database Configuration
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=airflow
POSTGRES_PORT=5432

# Airflow Web UI Admin User (created on first run)
_AIRFLOW_WWW_USER_CREATE=True
_AIRFLOW_WWW_USER_USERNAME=admin
_AIRFLOW_WWW_USER_PASSWORD=admin

# Airflow Database Migration (run on initialization)
_AIRFLOW_DB_UPGRADE=True

# AWS S3 Configuration for Patient Data Upload
# Change these values to match your AWS setup
AWS_S3_BUCKET=synthea-patient-data
AWS_S3_PREFIX=raw/fhir
AWS_CONN_ID=aws_default

# AWS Credentials (replace with your actual credentials)
# Format: aws://ACCESS_KEY_ID:SECRET_ACCESS_KEY@?region_name=REGION
# For IAM role: aws://@?region_name=us-east-1
# In CI/CD: Use GitHub Secrets to inject these values
AIRFLOW_CONN_AWS_DEFAULT=aws://YOUR_ACCESS_KEY_ID:YOUR_SECRET_ACCESS_KEY@?region_name=us-east-1

# Data Transformation Settings
# Set to 'true' to enable data transformations before S3 upload
ENABLE_TRANSFORMATIONS=false
